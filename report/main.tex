% Template for ICIP-2015 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,graphicx,todonotes}

\title{Image Classification Using SIFT Features and Artificial Neural Network}

\name{Dan Chianucci, Peter Muller}
\address{Rochester Institute of Technology\\
	Computer Engineering Department}


\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
In computer vision topics, classification is one of the more difficult actions that can be done with an image. Classification is the process of interpreting the contents of an image to sort it into one of several classes. In this experiment, SIFT, or Scale-Invariant Feature Transform, image features were extracted from images of the Caltech 101 data set. These features were quantized using K-means clustering, which were then used to train an artificial neural network (ANN) structure. \todo[inline]{results summary}
\end{abstract}
%
\section{Introduction}
\label{sec:intro}
\todo[inline]{Actual research into related works and the direction of our own experiment.}
%
\section{Methods}
\label{sec:methods}
As an overview of the experiment, several steps were taken. First, images from a single dataset were processed, and SIFT features were extracted from them. Preprocessing to normalize image intensities was not necessary because SIFT already accounted for these variations. An image compared with the same image, except for intensity variations, would still return the same SIFT features.

After every feature was collected from each image of the dataset, the data was quantized. K-means clustering was used to create bins for a histogram representation of the features. With a general structure that could be compared to for all images, the nearest-neighbor cluster centroids were used to count how many sift features appeared per histogram bin per image. These histograms gave the unique characteristics of each image class, which would be used for pattern recognition training in an artificial neural network.

A matrix of expected results was made for the training of an artificial neural network. In theory, the histograms of images in the same class should be very similar, so the ANN would be able to recognize these patterns and classify unknown images accordingly. Given these steps, the image classifications could be demonstrated.
%
\section{Experimental Setup} \todo[inline]{This is test protocol part}
\label{sec:expsetup}

In this experiment, five classes of images, trilobite, nautilus, scorpion, sea\_horse, and stegosaurus, were selected from the Caltech 101 image dataset \todo[inline]{citation}. Because these images were all from the same dataset, no dataset bias was introduced like it would be if images from different datasets were used for classification. 

To begin the classification process, features were extracted from the five selected classes of images. For each image, SIFT features were collected into an overlying data structure which contained the SIFT features from all images. As mentioned previously, preprocessing of images was not required because the SIFT features would not change based on different image intensities.

After the collection of all SIFT features, K-means clustering was used to quantize the features database into a structure that individual image histograms would be compared to. \todo[inline]{Explain multiple K values} Because of the clustering, the mean centroids could be used to associate SIFT features of an individual image to the rest of the dataset. Using the nearest-neighbor centroid to the SIFT features of each individual image, histograms were calculated, denoting how many features were associated with a specific centroid. These histograms were used as inputs for the training and classification of images within the artificial neural network structure.

Of the 341 images investigated in this experiment, \todo[inline]{number of images} were used for training, \todo[inline]{another number of images} were used for verification of the training, and \todo[inline]{last number of images} were used for the actual classification experiment. Using the feature histograms from each image and a matrix of expected classifications, the artificial neural network was trained to identify and classify the different image classes. The Neural Net Pattern Recognition application from the Neural Network Toolbox in MATLAB was used for the training and creation of the ANN, which was used to retrieve the results of the classifications.

%
\section{Results}
\label{sec:results}
Upon training the ANN, it was apparent that parameters of the experiment had to be altered to achieve the highest possible accuracy of classification. After fixing the number of hidden layers in the neural network to \todo[inline]{number of layers} layers, the only variable to adjust was the number of bins in the histograms generated using the K-means quantization. Figure \todo[inline]{insert a figure here} shows a comparison of the classification accuracies of different K-values used during the quantization stage.

Once the greatest accuracy configuration was established, the images were then classified. For each image class, the best-case matches and the best-matching false positive results are shown in Figure \todo[inline]{another figure}.

%
%sample figure
%\begin{figure}[Ht]
%\centering
%\includegraphics[scale=0.75]{Figures/objects.png}
%\caption{Subset of object skeletonizations \cite{skeletons}.}
%\label{fig:objects}
%\end{figure}


%
\section{Discussion}
\label{sec:discussion}
One of the major factors of the accuracy of a neural network classifiers was the amount of training data available for it to consume. Because only five classes were chosen for processing, that greatly reduced the number of images that could be used. Considering other implementations of neural networks, the lack of a large dataset was compensated by using a larger number of hidden layers within the ANN itself. Most networks only contained two to three layers, but this one contained \todo[inline]{number of layers}.

Because of the fixed number of hidden layers, the K-value, used for clustering, was varied to achieve the greatest accuracy of the network, given little training data to work with. As shown in Figures \todo[inline]{figure numbers}, the K-value of \todo[inline]{number} gave the highest accuracy. \todo[inline]{talk about trend of K-values}

\todo[inline]{talk about image results}

\section{Appendix}
\label{sec:appendix}
\todo[inline]{include matlab code.}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: refs). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
%\bibliographystyle{IEEEbib}
%\bibliography{refs}

\end{document}
